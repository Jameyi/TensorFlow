import tensorflow as tf
import numpy as np
import os
import csv
from tensorflow.examples.tutorials.mnist.import input_data

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

#设置算法超参数
learning_rate_init = 0.001
training_epochs = 1
batch_size = 100
display_step = 10

# Network Parameters
# MNIST data input(img shape:28*28)
n_input = 784 
# MNIST total classes(0-9 digits)
n_classes = 10

# return initialed weights of name base on speicify dims
def WeightsVariable(shape,name_str,stddev=0.1):
	initial = tf.random_normal(shape=shape,stddev=stddev,dtype=tf.float32)
	#initial = tf.truncated_normal(shape=shape,stddev=stddev,dtype=tf.float32)
	return tf.Variable(initial,dtype=tf.float32,name=name_str)

# return initialed bias of name base on specify dims
def BiasesVariable(shape,name_str,stddev=0.00001):
	initial = tf.random_normal(shape=shape,stddev=stddev,dtype=tf.float32)
	#initial = tf.constant(stddev,shape=shape)
	return tf.Variable(initial,dtype=tf.float32,name=name_str)
	
def Conv2d(x,W,b,stride=1,padding='SAME'):
	with tf.name_scope('Wx_b'):
		y = tf.nn.conv2d(x,W,stride=[1,stride,stride,1],padding=padding)
		y = tf.nn.bias_add(y,b)
	return y

def Activation(x,activation=tf.nn.relu,name='relu'):
	with tf.name.scope(name):
		y=activation(x)
	return y

def Pool2d(x,pool=tf.nn.max_pool,k=2,stride=2):
	return pool(x,ksize=[1,k,k,1],stride=[1,stride,stride,1],padding='VALID')

def FullyConnected(x,W,b,activate=tf.nn.relu,act_name='relu'):
	with tf.name_scope('Wx_b'):
		y=tf.matmul(x,W)
		y=tf.add(y,b)
	with tf.name_scope(act_naame):
		y=activate(y)
	return y

with tf.Graph().as_default():
	# input of graph
	with tf.name_scope('Inputs'):
		X_origin = tf.placeholder(tf.float32,[None,n_input],name='X_origin')
		y_true = tf.placeholder(tf.float32,[None,n_classes],name='Y_true')
		# reshape the image data from N*784 to N*28*28*1 tensor
		X_image = tf.reshape(X_origin,[-1,28,28,1])
	# 计算图前向推断过程
	with tf.name_scope('Inference'):
		# first conv layer(conv2d + bias)
		with tf.name_scope('Conv2d'):
			weights = WeightsVariable(shape=[5,5,1,16],name_str='weights')
			biases = BiasesVariable(shape=[16],name_str='biases')
			conv_out = Conv2d(X_image,weights,biases,stride=1,padding='VALID')
		# nonliner activation layer
		with tf.name_scope('Activate'):
			activate_out = Activation(conv_out,activation=tf.nn.relu,name='relu')
		# the first pooling layer
		with tf.name_scope('Pool2d'):
			pool_out = Pool2d(activate_out,pool=tf.nn.max_pool,k=2,stride=2)
		# change 2 dims features to 1 dim vector
		with tf.name_scope('FeatsReshape'):
			features = tf.reshape(pool_out,[-1,12*12*16])
		# the first fully connection layer
		with tf.name_scope('FC_Linear'):
			weights = WeightsVariable(shape=[12*12*16,n_classes],name_str='weights')
			biases = BiasesVariable(shape=[n_classes],name_str='biases')
			Ypred_logits = FullyConnected(features,weights,biases,activate=tf.identity,act_name='identity')
	
	with tf.name_scope('Loss'):
		cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_true,logits=Y_pred_logits))
	
	with tf.name_scope('Train'):
		learning_rate = tf.placeholder(tf.float32)
		optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
		trainer = optimizer.minimize(cross_entropy_loss)
		
	with tf.name_scope('Evaluate'):
		correct_pred = tf.equal(tf.argmax(Ypred_logits,1),tf.argmax(Y_true,1))
		accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))
		
	init = tf.global_variables_initializer()
	
	print("write graphy to event document,check in tensorbroad")
	summary_writer = tf.summary.FileWriter(logdir='logs/excise311/',graph=tf.get_default_graph())
	summary_writer.close()
	
	# 加载数据集
	mnist = input_data.read_data_sets("../MNIST_data/",one_hot=True)
	# 将评估结果保存到文件
	results_list = list()
	# 写入参数配置
	results_list.append(['learning_rate',learning_rate_init,
			     'traning_epochs',training_epochs,
			     'batch_size',batch_size,
			     'display_step',display_step])
	results_list.append(['train_step','train_loss','validation_loss','train_step','train_accuracy','validation_accuracy'])
	# 启动计算图
	with tf.Session() as sess:
		sess.run(init)
		total_batches = int(mnist.train.num_examples/batch_size)
		print("Per batch size:",batch_size)
		print("Train sample count:",mnist.train.num_examples)
		print("Total batch count:",total_batches)
		trainning_step = 0  #记录模型被训练的步数
		# 训练指定轮数，每一轮所有训练样本都要过一遍
		for epoch in range(training_epochs):
			# 每一轮都要把所有的batch跑一遍
			for batch_idx in range(total_batches):
				# 取出数据
				batch_x,batch_y = mnist.train.next_batch(batch_size)
				# 运行优化器训练节点(backprop)
				sess.run(trainer,feed_dict={X_origin:batch_x,Y_true:batch_y,learning_rate:learning_rate_init})
				# 每调用一次训练节点，training_step就加1，最终==training_epochs*total_batch
				training_step += 1
				
				# 每训练display_step次，计算当前模型的损失和分类准确度
				if training_step % display_step == 0:
					# 计算当前模型在目前（最近）见过的display_step个batchsize的训练集上的损失和分类准确率
					start_idx = max(0,(batch_idx-display_step)*batch_size)
					end_idx = batch_idx*batch_size
					train_loss,train_acc = EvaluateModelOnDataset(sess,mnist.train.images[start_idx:end_idx,:],
																	   mnist.train.labels[start_idx:end_idx,:])
					print("Training Step:" + str(training_step) +
					      ",Training Loss=" + "{:,6f}".format(train_loss) +
						  ",Training Accuracy=" + "{:,5f}.format(train_acc))
					
					# 计算当前模型在验证集的损失和分类准确率
					validation_loss,validation_acc = EvaluateModelOnDataset(sess,
																			mnist.validation.images,
																			mnist.validation.labels)
					print("Training Step:" + str(training_step) +
					      ",Validation Loss=" + "{:,6f}".format(validation_loss) +
						  ",Validation Accuracy=" + "{:,5f}.format(validation_acc))
					
	






















